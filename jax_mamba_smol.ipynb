{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHLC8TgF9KEh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import jax\n",
        "import jax.nn as nn\n",
        "import jax.numpy as jnp\n",
        "import jax.lax as lax\n",
        "import jax.tree as jt\n",
        "import jax.random as random\n",
        "\n",
        "from jax import jit, value_and_grad, vmap, Array\n",
        "from einops import repeat, einsum\n",
        "from operator import getitem\n",
        "from functools import partial\n",
        "from typing import Union, NamedTuple\n",
        "from dataclasses import dataclass\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4bu2NP79Tu9"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ModelArgs:\n",
        "    d_model: int\n",
        "    n_layer: int\n",
        "    vocab_size: int\n",
        "    d_state: int = 16\n",
        "    expand: int = 2\n",
        "    dt_rank: Union[int, str] = 'auto'\n",
        "    dt_min: float = 0.001\n",
        "    dt_max: float = 0.1\n",
        "    dt_scale = 1.0\n",
        "    dt_init_floor = 1e-4\n",
        "    d_conv: int = 4\n",
        "    pad_vocab_size_multiple: int = 8\n",
        "    conv_bias: bool = True\n",
        "    bias: bool = False\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.d_inner = self.expand * self.d_model\n",
        "\n",
        "        if self.dt_rank == 'auto':\n",
        "            self.dt_rank = math.ceil(self.d_model / 16)\n",
        "        \n",
        "        self.orig_vocab_size = self.vocab_size\n",
        "\n",
        "        if self.vocab_size % self.pad_vocab_size_multiple != 0:\n",
        "            self.vocab_size += self.pad_vocab_size_multiple - self.vocab_size % self.pad_vocab_size_multiple\n",
        "\n",
        "\n",
        "class LayerParams(NamedTuple):\n",
        "    norm: Array\n",
        "    in_proj: Array\n",
        "    in_proj_bias: Union[None, Array]\n",
        "    conv: Array\n",
        "    conv_bias: Union[None, Array]\n",
        "    x_proj: Array\n",
        "    dt_proj: Array\n",
        "    dt_proj_bias: Array\n",
        "    A_log: Array\n",
        "    D: Array\n",
        "    out_proj: Array\n",
        "    out_proj_bias: Union[None, Array]\n",
        "\n",
        "\n",
        "class MambaParams(NamedTuple):\n",
        "    embedding: Array\n",
        "    layers: LayerParams\n",
        "    norm_f: Array\n",
        "\n",
        "\n",
        "def initialize_params(key, args):\n",
        "    truncated_normal_stddev = .87962566103423978\n",
        "\n",
        "    d_model_scale = 1 / (math.sqrt(args.d_model) * truncated_normal_stddev)\n",
        "    d_inner_scale = 1 / (math.sqrt(args.d_inner) * truncated_normal_stddev)\n",
        "    dt_rank_scale = 1 / (math.sqrt(args.dt_rank) * truncated_normal_stddev)\n",
        "    dt_init_std = args.dt_rank ** -0.5 * args.dt_scale\n",
        "\n",
        "    embed_key, dt_key, layers_key = random.split(key, 3)\n",
        "    layers_keys = random.split(layers_key, 7)\n",
        "\n",
        "    embedding = random.truncated_normal(embed_key, -2, 2, (args.vocab_size, args.d_model)) * d_model_scale\n",
        "\n",
        "    dt = jnp.exp(\n",
        "        random.uniform(dt_key, (args.n_layer, args.d_inner)) \\\n",
        "            * (math.log(args.dt_max) - math.log(args.dt_min)) + math.log(args.dt_min)\n",
        "    ).clip(min=args.dt_init_floor)\n",
        "\n",
        "    layers = LayerParams(\n",
        "        norm=jnp.ones((args.n_layer, args.d_model)),\n",
        "\n",
        "        in_proj=random.truncated_normal(\n",
        "            layers_keys[0], -2, 2, (args.n_layer, args.d_model, args.d_inner * 2)\n",
        "        ) * d_model_scale,\n",
        "        in_proj_bias=jnp.zeros((args.n_layer, args.d_inner)) if args.bias else None,\n",
        "\n",
        "        conv=random.truncated_normal(\n",
        "            layers_keys[1], -2, 2, (args.n_layer, args.d_inner, args.d_conv)\n",
        "        ) * d_inner_scale,\n",
        "        conv_bias=jnp.zeros((args.n_layer, args.d_inner)) if args.conv_bias else None,\n",
        "\n",
        "        x_proj=random.truncated_normal(\n",
        "            layers_keys[2], -2, 2, (args.n_layer, args.d_inner, args.dt_rank + 2 * args.d_state)\n",
        "        ) * d_inner_scale,\n",
        "\n",
        "        dt_proj=random.uniform(\n",
        "            layers_keys[3],\n",
        "            (args.n_layer, args.dt_rank, args.d_inner),\n",
        "            minval=-dt_init_std,\n",
        "            maxval=dt_init_std\n",
        "        ),\n",
        "        dt_proj_bias=dt + jnp.log(-jnp.expm1(-dt)),\n",
        "\n",
        "        A_log=repeat(\n",
        "            jnp.log(jnp.arange(1, args.d_state + 1)),\n",
        "            'ds -> nl di ds',\n",
        "            nl=args.n_layer,\n",
        "            di=args.d_inner\n",
        "        ),\n",
        "        D=jnp.ones((args.n_layer, args.d_inner)),\n",
        "\n",
        "        out_proj=random.truncated_normal(\n",
        "            layers_keys[4], -2, 2, (args.n_layer, args.d_inner, args.d_model)\n",
        "        ) * d_inner_scale,\n",
        "        out_proj_bias=jnp.zeros((args.n_layer, args.d_model)) if args.bias else None,\n",
        "    )\n",
        "\n",
        "    norm_f = jnp.ones(args.d_model)\n",
        "\n",
        "    return MambaParams(embedding=embedding, layers=layers, norm_f=norm_f)\n",
        "\n",
        "\n",
        "def zero_or(x):\n",
        "    return 0 if x is None else x\n",
        "\n",
        "\n",
        "def rms_norm(w, x, eps):\n",
        "    z = x.astype(jnp.float32)\n",
        "    norm = z * lax.rsqrt(jnp.mean(z * z, -1, keepdims=True) + eps)\n",
        "    return w * norm.astype(x.dtype)\n",
        "\n",
        "\n",
        "# training\n",
        "def mamba(args, use_associative_scan, params, tokens):\n",
        "\n",
        "    def block(x, params):\n",
        "        # (l, d_model) -> (l, d * 2) -> (l, d), (l, d)\n",
        "        x, res = jnp.split(x @ params.in_proj + zero_or(params.in_proj_bias), 2, -1)\n",
        "        # (l, d) -> (l + c - 1, d) -> (d, l + c - 1)\n",
        "        x = jnp.concatenate([jnp.zeros((args.d_conv - 1, args.d_inner)), x], 0).T\n",
        "        # (d, l + c - 1) -> (d, l) -> (l, d)\n",
        "        x = vmap(jnp.convolve, (0, 0, None))(x, params.conv, 'valid').T + zero_or(params.conv_bias)\n",
        "        x = nn.silu(x)\n",
        "        # (l, d) -> (l, r + s + s) -> (l, r), (l, s), (l, s)\n",
        "        x_dt, B, C = jnp.split(x @ params.x_proj, [args.dt_rank, args.dt_rank + args.d_state], -1)\n",
        "        # (l, r) -> (l, d)\n",
        "        dt = nn.softplus(x_dt @ params.dt_proj + zero_or(params.dt_proj_bias))\n",
        "        # discretized A and B\n",
        "        dA = jnp.exp(einsum(dt, -jnp.exp(params.A_log), 'l d, d s -> l d s'))\n",
        "        dBx = einsum(x * dt, B, 'l d, l s -> l d s')\n",
        "        # see section 1.4.1 \"First-Order Recurrences\" in the paper \"Prefix Sums and Their Applications\"\n",
        "        # the main loop is equivalent to\n",
        "        # \n",
        "        # ssm_states = []\n",
        "        # s = jnp.zeros((args.d_inner, args.d_state))\n",
        "        # for c in zip(dA, dBx):\n",
        "        #     s = c[0] * s + c[1]\n",
        "        #     ssm_states.append(s)\n",
        "        # ssm_states = jnp.stack(ssm_states)\n",
        "        #\n",
        "        # we use the associative operator `op` below to parallelize this\n",
        "        if use_associative_scan:\n",
        "            op = lambda s, c: (c[0] * s[0], c[0] * s[1] + c[1])\n",
        "            _, ssm_states = lax.associative_scan(op, (dA, dBx))\n",
        "        # or we can implement the same loop using lax.scan \n",
        "        else:\n",
        "            def op(s, c):\n",
        "                s = c[0] * s + c[1]\n",
        "                return s, s\n",
        "\n",
        "            _, ssm_states = lax.scan(op, jnp.zeros((args.d_inner, args.d_state)), (dA, dBx))\n",
        "        # read out, gating, then output projection\n",
        "        y = einsum(ssm_states, C, 'l d s, l s -> l d') + x * params.D\n",
        "        y = y * nn.silu(res)\n",
        "        # (l, d) -> (l, d_model)\n",
        "        return y @ params.out_proj + zero_or(params.out_proj_bias)\n",
        "\n",
        "    def f(x, params):\n",
        "        return x + block(rms_norm(params.norm, x, 1e-8), params), None\n",
        "\n",
        "    h, _ = lax.scan(f, params.embedding[tokens], params.layers)\n",
        "    \n",
        "    logits = rms_norm(params.norm_f, h, 1e-8) @ params.embedding.T\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "# inference\n",
        "def mamba_step(args, valid_logits, params, cache, token):\n",
        "\n",
        "    def block(x, params, conv_cache, ssm_state):\n",
        "        x, res = jnp.split(x @ params.in_proj + zero_or(params.in_proj_bias), 2, -1)\n",
        "        # convolve input with kernel\n",
        "        conv_input = jnp.concatenate([conv_cache, x[:, None]], -1)  # (d_inner, d_conv)\n",
        "        kernel = jnp.flip(params.conv, -1)  # (d_inner, d_conv)\n",
        "        x = nn.silu(jnp.vecdot(conv_input, kernel) + zero_or(params.conv_bias))\n",
        "        # per token discretization, read-in, and read-out vectors\n",
        "        x_dt, B, C = jnp.split(x @ params.x_proj, [args.dt_rank, args.dt_rank + args.d_state], -1)\n",
        "        # dt should always be positive\n",
        "        dt = nn.softplus(x_dt @ params.dt_proj + zero_or(params.dt_proj_bias))\n",
        "        # (s,) -> (1, s), (d,) -> (d, 1)\n",
        "        B, dt = B[None], dt[:, None]\n",
        "        # (d, s)\n",
        "        ssm_state = jnp.exp(-jnp.exp(params.A_log) * dt) * ssm_state + B * x[:, None] * dt\n",
        "        # (d, s) @ (s,).T + (d,) * (d,) -> (d,)\n",
        "        y = ssm_state @ C.T + x * params.D\n",
        "        # gating, output projection, then return with cache\n",
        "        y = y * nn.silu(res)\n",
        "        y = y @ params.out_proj + zero_or(params.out_proj_bias)\n",
        "        return y, (conv_input[:, 1:], ssm_state)\n",
        "\n",
        "    def f(x, params_and_cache):\n",
        "        params, cache = params_and_cache\n",
        "        h, cache = block(rms_norm(params.norm, x, 1e-8), params, *cache)\n",
        "        return x + h, cache\n",
        "\n",
        "    h, cache = lax.scan(f, params.embedding[token], (params.layers, cache))\n",
        "\n",
        "    logits = rms_norm(params.norm_f, h, 1e-8) @ params.embedding.T\n",
        "    \n",
        "    return logits[:args.orig_vocab_size if valid_logits else args.vocab_size], cache\n",
        "\n",
        "\n",
        "def adam(lr, b1, b2, eps, step, params, grads, state):\n",
        "    m, v = state\n",
        "    m = jt.map(lambda m, g: b1 * m + (1 - b1) * g, m, grads)\n",
        "    v = jt.map(lambda v, g: b2 * v + (1 - b2) * g ** 2, v, grads)\n",
        "    m_ = jt.map(lambda m: m / (1 - b1 ** step), m)\n",
        "    v_ = jt.map(lambda v: v / (1 - b2 ** step), v)\n",
        "    params = jt.map(lambda p, m, v: p - lr * m / (v + eps) ** .5, params, m_, v_)\n",
        "    return params, (m, v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5ysVJ1B1DRU",
        "outputId": "314ba310-8603-4435-effb-0717975f2477"
      },
      "outputs": [],
      "source": [
        "# !rm *.md\n",
        "!wget \"https://raw.githubusercontent.com/textvs/Austen-Works/master/1813%20%7C%20Pride%20and%20Prejudice%20(PG%201342)/Jane%20Austen%2C%20Pride%20and%20Prejudice%20(1813).md\" \\\n",
        "      \"https://raw.githubusercontent.com/textvs/Austen-Works/master/1811%20%7C%20Sense%20and%20Sensibility%20(PG%2021839)/Jane%20Austen%2C%20Sense%20and%20Sensibility%20(1811).md\" \\\n",
        "      \"https://raw.githubusercontent.com/textvs/Austen-Works/master/1818%20%7C%20Persuasion%20(PG%20105)/Jane%20Austen%2C%20Persuasion%20(1818).md\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chOl9GJEyzXf",
        "outputId": "ed42068d-9198-4f22-9fa4-7799a68ca728"
      },
      "outputs": [],
      "source": [
        "text = '\\n[end]\\n'.join([open(f, 'r').read() for f in os.listdir() if f.endswith('.md')])\n",
        "\n",
        "vocab = ['<|pad|>'] + sorted(list(set(text)))\n",
        "print(f'vocabulary:\\n{vocab}')\n",
        "\n",
        "itoc = {i: c for i, c in enumerate(vocab)}\n",
        "ctoi = {c: i for i, c in enumerate(vocab)}\n",
        "\n",
        "encode = lambda string: [ctoi[c] for c in string]\n",
        "decode = lambda tokens: ''.join([itoc[i] for i in tokens])\n",
        "\n",
        "tokens = jnp.array(encode(text))\n",
        "print(f'# tokens:\\n{len(tokens)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cross_entropy(logits, targets):\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    targets = targets.reshape(-1)\n",
        "    return (nn.logsumexp(logits, -1) - vmap(getitem)(logits, targets)).mean()\n",
        "\n",
        "\n",
        "def get_sampler(tokens, batch_size, seq_len):\n",
        "\n",
        "    def sampler(key):\n",
        "        start = random.randint(key, (batch_size, 1), 0, len(tokens) - seq_len - 1)\n",
        "        batch = tokens[jnp.arange(seq_len + 1) + start]\n",
        "        return batch[:, :-1], batch[:, 1:]\n",
        "    \n",
        "    return sampler\n",
        "\n",
        "\n",
        "def get_train_step(model, optimizer, sampler):\n",
        "\n",
        "    def loss_fn(params, inputs, targets):\n",
        "        return cross_entropy(model(params, inputs), targets)\n",
        "    \n",
        "    def train_step(key, step, params, state):\n",
        "        loss, grads = value_and_grad(loss_fn)(params, *sampler(key))\n",
        "        params, state = optimizer(step, params, grads, state)\n",
        "        return loss, params, state\n",
        "    \n",
        "    return jit(train_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "args = ModelArgs(d_model=384, n_layer=6, vocab_size=len(vocab))\n",
        "use_associative_scan = True\n",
        "model = vmap(partial(mamba, args, use_associative_scan), (None, 0))\n",
        "\n",
        "lr = 1e-3\n",
        "b1 = 0.9\n",
        "b2 = 0.999\n",
        "eps = 1e-8\n",
        "optimizer = partial(adam, lr, b1, b2, eps)\n",
        "\n",
        "batch_size = 8\n",
        "seq_len = 512\n",
        "sampler = get_sampler(tokens, batch_size, seq_len)\n",
        "\n",
        "train_step = get_train_step(model, optimizer, sampler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "key, subkey = random.split(random.key(42))\n",
        "params = initialize_params(subkey, args)\n",
        "state = jt.map(jnp.zeros_like, (params, params))\n",
        "sum(p.size for p in jt.leaves(params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "DRYrMX1CADPz",
        "outputId": "0ee2f7a9-218b-42ee-ed99-ade12bf4002d"
      },
      "outputs": [],
      "source": [
        "steps = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "rSSla-CeoJoT",
        "outputId": "ba310c63-5899-4e3d-b281-034fa0410512"
      },
      "outputs": [],
      "source": [
        "losses = []\n",
        "\n",
        "for step in range(1, steps + 1):\n",
        "    key, subkey = random.split(key)\n",
        "    loss, params, state = train_step(subkey, step, params, state)\n",
        "    losses.append(loss)\n",
        "    print(f'step {step:4d} loss {float(loss):11.7f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLk7z3xtj8Tz"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qG7giSbhpZvY"
      },
      "outputs": [],
      "source": [
        "def run(key, args, params, prompt, steps, temperature=1):\n",
        "    f = jit(partial(mamba_step, args, True, params))\n",
        "\n",
        "    tokens = jnp.array(encode(prompt))\n",
        "\n",
        "    cache = (\n",
        "        jnp.zeros((args.n_layer, args.d_inner, args.d_conv - 1)),  # conv_cache\n",
        "        jnp.zeros((args.n_layer, args.d_inner, args.d_state))  # ssm_state\n",
        "    )\n",
        "\n",
        "    # step through prompt tokens once to get next token and current state \n",
        "    for token in tokens:\n",
        "        logits, cache = f(cache, token)\n",
        "\n",
        "    print(prompt, end='')\n",
        "  \n",
        "    token = random.categorical(key, logits / temperature)\n",
        "    print(itoc[int(token)], end='')\n",
        "\n",
        "    # sample tokens autoregressively \n",
        "    for _ in range(steps):\n",
        "        key, subkey = random.split(key)\n",
        "        logits, cache = f(cache, token)\n",
        "        token = random.categorical(subkey, logits / temperature)\n",
        "        print(itoc[int(token)], end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFdI9An1vD3Q"
      },
      "outputs": [],
      "source": [
        "key = random.key(42)\n",
        "prompt = \"Mr. Bennet was among the\"\n",
        "steps = seq_len - len(prompt)\n",
        "run(key, args, params, prompt, steps)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03fa48b712e74d8984109ac8c27bece0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16c17e69d361485dbaff1bbe7349f035": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03fa48b712e74d8984109ac8c27bece0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f5783119bc345e388d69b9a8fdcce3e",
            "value": 1
          }
        },
        "25bc0f23a16e49b0b124e32c1a310909": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5468efd92a8d4917ab2509c236fe06fc",
              "IPY_MODEL_16c17e69d361485dbaff1bbe7349f035"
            ],
            "layout": "IPY_MODEL_506787a4b09347edb23452dd9e48a51c"
          }
        },
        "4abf31fa784a499bbbf3e46133b200e1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "506787a4b09347edb23452dd9e48a51c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5468efd92a8d4917ab2509c236fe06fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4abf31fa784a499bbbf3e46133b200e1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_aff53c875f20453386e9513b9332a378",
            "value": "0.010 MB of 0.010 MB uploaded\r"
          }
        },
        "9f5783119bc345e388d69b9a8fdcce3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aff53c875f20453386e9513b9332a378": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
